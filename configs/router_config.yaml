# Model configuration
model:
  base_model_id: "gpt2"  # Using a small model for testing
  torch_dtype: "float32"
  device_map: "auto"

# Router configuration
router:
  hidden_size: 768  # Should match the base model's hidden size
  num_experts: 4
  expert_selection_threshold: 0.1
  max_experts_per_token: 2
  capacity_factor: 1.25
  use_router_bias: True
  load_balancing_weight: 0.01  # Weight for the load balancing loss
  router_dropout: 0.1  # Dropout probability for router layers
  router_type: "threshold"  # Type of router to use

# Training configuration
training:
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  num_train_epochs: 3
  learning_rate: 1e-4
  warmup_ratio: 0.1
  weight_decay: 0.01
  max_seq_length: 128
  logging_steps: 10
  eval_steps: 100
  save_total_limit: 2
  seed: 42
